{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Tool to Analyze UFS Input & Baseline Datasets\n",
    "\n",
    "### __Purpose:__ \n",
    "\n",
    "The purpose of the tool is to extract additional information regarding the input and baseline datasets residing within the RDHPCS -- in an effort to assist NOAA in the cleaning & restructuring of the current UFS data structure. In this demontration, the tool will be applied against the UFS input and baseline datasets residing within the RDHPC platform, Orion.\n",
    "\n",
    "### __Capabilities:__ \n",
    "\n",
    "The tool will be able to perform the following actions:\n",
    "\n",
    "- Apply feature engineering to obtain additional information regarding the data files. \n",
    "  \n",
    "- Extract all data filenames mapping them to their corresponding relative directory path, \n",
    "  root folder, filename, filesize, file format, compiler (if applicable), CNTL folder (if applicable),\n",
    "  \"input\" or \"restart\" file type, resolution (km) category, resolution (C resolution) category. \n",
    "  \n",
    "- Analyze & plot the current storage size of each UFS data file per timestamp per dataset type (input, baseline). \n",
    "  Note: At the time at which the tool is being executed, it will extract the data information from the RDHPC\n",
    "  platform at that time. Thus, the data information of the datasets residing within the RDHPC will be dynamic \n",
    "  as the datasets are being cleaned.\n",
    "  \n",
    "- Extract number of nodes (\"folders\") per file to assist in categorization of the folders -- currently, node levels vary.\n",
    "- Confirm no input data files are being duplicated per UFS Component's root folder and per CNTL folder.\n",
    "- Categorize, sort by feature, & write restructuring data table to csv file.\n",
    "\n",
    "### __Future Capabilities:__  \n",
    "- This tool can be used as a skeleton framework for acquiring additional information regarding the data analytics of future datasets of interest (e.g. SRW data, MRW data, etc).\n",
    "- Construct the hierarchical trees for each unique dataset residing within a RDHPC platform.\n",
    "\n",
    "### __Procedural Steps to Utilize Data Analytics Tool:__\n",
    "\n",
    "#### ___Environment Setup___\n",
    "\n",
    "1.\tInstall miniconda on your machine. Note: Miniconda is a smaller version of Anaconda that only includes conda along with a small set of necessary and useful packages. With Miniconda, you can install only what you need, without all the extra packages that Anaconda comes packaged with:\n",
    "\n",
    "Download latest Miniconda (e.g. 3.9 version):\n",
    "__wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Check integrity downloaded file with SHA-256:\n",
    "__sha256sum Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Reference SHA256 hash in following link: https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "Install Miniconda in Linux:\n",
    "__bash Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Next, Miniconda installer will prompt where do you want to install Miniconda. Press ENTER to accept the default install location i.e. your $HOME directory. If you don't want to install in the default location, press CTRL+C to cancel the installation or mention an alternate installation directory. If you've chosen the default location, the installer will display “PREFIX=/var/home/<user>/miniconda3” and continue the installation.\n",
    "\n",
    "For installation to take into effect, run the following command: \n",
    "__source ~/.bashrc__\n",
    "\n",
    "You will now see the prefix (base) in front of your shell prompt. It means that the conda's base environment is activated.\n",
    "\n",
    "2.\tOnce you have conda installed on your machine, perform the following to create a conda environment:\n",
    "\n",
    "    - To rely on the base Miniconda installation:\n",
    "    __conda create -n [Name of your conda environment you wish to create]__\n",
    "    \n",
    "    (or)\n",
    "\n",
    "    - To ensure you are running Python 3.9:\n",
    "    __conda create -n myenv Python=3.9__\n",
    "\n",
    "From now on, you have to manually activate conda environment using command: __$ conda activate__\n",
    "To deactivate conda environment: __$ conda deactivate__\n",
    "\n",
    "\n",
    "3.\tA .yml file is a text file that contains a list of dependencies, which channels a list for installing dependencies. To build the created conda environment from YAML file execute __conda env create -f environment.yml__\n",
    "\n",
    "__*Note:__ For the code to utilize the dependencies, you will need to be in the directory where the environment.yml file lives.\n",
    "\n",
    "4.\tActivate the new environment via: \n",
    "__conda activate [Name of your conda environment you wish to activate]__\n",
    "\n",
    "5.\tVerify that the new environment was installed correctly via:\n",
    "__conda info –envs__\n",
    "\n",
    "\n",
    "#### ___Link Home Directory to Dataset Location on RDHPCS Platform___ \n",
    "6.\tUnfortunately, there is no way to navigate to the /work/ filesystem from within the Jupyter interface. The best way to workaround is to create a symbolic link in your home folder that will take you to the /work/ filesystem. Run the following command from a linux terminal on Orion to create the link: \n",
    "\n",
    "__ln -s /work /home/[Your user account name]/work__\n",
    "\n",
    "Now, when you navigate to the /home/[Your user account name]/work directory in Jupyter, it will take you to the /work/ directory. Allowing you to obtain any data in the /work/ filesystem that you have permission to access from Jupyter. This same procedure will work for any filesystem available from the root directory. \n",
    "\n",
    "__Note:__ On Orion, user must sym link from their home directory to the main directory containing the datasets of interest.\n",
    "\n",
    "___Open & Run Data Analytics Tool on Jupyter Notebook___\n",
    "7.\tOpen OnDemand has a built-in file explorer and file transfer application available directly from its dashboard via ...\n",
    "    - Login to https://orion-ood.hpc.msstate.edu/ \n",
    "    - In the Open OnDemand Interface, select __Interactive Apps__ > __Jupyter Notbook__\n",
    "    - Set the following configurations to run Jupyter:\n",
    "\n",
    "\n",
    "#### ___Additonal Information:___\n",
    "\n",
    "__To create a .yml file__\n",
    "\n",
    "- Activate the environment to export: \n",
    "__conda activate myenv__\n",
    "\n",
    "- Export your active environment to a new file:\n",
    "__conda env export > environment.yml__\n",
    "\n",
    "\n",
    "### __Reference(s):__\n",
    "- Latest UFS Weather Model Guide:\n",
    "    - https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "class read_data_dirs():\n",
    "    \"\"\"\n",
    "    Extracts all data (residing on a RDHPCS) filenames & map them to their corresponding \n",
    "    directories. Applies feature engineering and analyzes the current data structure of the datasets\n",
    "    being utilized for the UFS weather model repository. Note: Can be modified for \n",
    "    the data analytics of future datasets of interest (e.g. SRW data, MRW data, etc). \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hpc_dir, avoid_fldrs):\n",
    "        \n",
    "        # Datasets' main directory of interest. \n",
    "        self.hpc_dir = hpc_dir\n",
    "        \n",
    "        # Extracts all directories residing w/in datasets' main directory of interest. \n",
    "        # Remove file directories comprise of a folder name.\n",
    "        self.avoid_fldrs = avoid_fldrs\n",
    "        self.file_dirs = self.get_data_dirs()\n",
    "        \n",
    "        # List of all directories w/in the input & baseline datasets.\n",
    "        self.input_dirs, self.baseline_dirs = self.get_input_bl_data()\n",
    "        \n",
    "        # File size.\n",
    "        self.input_filesize = self.get_filesize(\"input\")\n",
    "        self.baseline_filesize = self.get_filesize(\"baseline\")\n",
    "        \n",
    "        # List of all data folders/files in datasets' main directory of interest.\n",
    "        print(\"\\033[1m\" +\\\n",
    "              f\"All Primary Dataset Folders & Files In Main Directory ({self.hpc_dir}):\" +\\\n",
    "              f\"\\n\\n\\033[0m{os.listdir(self.hpc_dir)}\")\n",
    "        \n",
    "    def get_data_dirs(self):\n",
    "        \"\"\"\n",
    "        Extract list of all file directories in datasets' main directory.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Return (list): List of all file directories in datasets' main directory\n",
    "        of interest.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate list of all file directories residing w/in datasets' \n",
    "        # main directory of interest. \n",
    "        file_dirs = []\n",
    "        file_size =[]\n",
    "        for root_dir, subfolders, filenames in os.walk(self.hpc_dir):\n",
    "            for file in filenames:\n",
    "                file_dirs.append(os.path.join(root_dir, file))\n",
    "        \n",
    "        # Removal of personal names.\n",
    "        if self.avoid_fldrs != None:\n",
    "            file_dirs = [x for x in file_dirs if any(x for name in self.avoid_fldrs if name not in x)]\n",
    "        \n",
    "        return file_dirs\n",
    "\n",
    "    def get_input_bl_data(self):\n",
    "        \"\"\"\n",
    "        Extract list of all input file & baseline file directories.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Return (list, list): List of all input file & baseline file directories.\n",
    "\n",
    "        \"\"\"\n",
    "        input_dirs = []\n",
    "        baseline_dirs = []\n",
    "        for file_dir in self.file_dirs:\n",
    "\n",
    "            # Input data files w/ root directory truncated.\n",
    "            if any(subfolder in file_dir for subfolder in ['input-data']):\n",
    "                input_dirs.append(file_dir.replace(self.hpc_dir, \"\"))\n",
    "\n",
    "            # Baseline data files w/ root directory truncated.\n",
    "            if any(subfolder in file_dir for subfolder in ['develop', 'ufs-public-release']):\n",
    "                baseline_dirs.append(file_dir.replace(self.hpc_dir, \"\"))\n",
    "\n",
    "        return input_dirs, baseline_dirs    \n",
    "    \n",
    "    def get_filesize(self, dataset_type):\n",
    "        \"\"\"\n",
    "        Extracts storage size for each data file located in either input or baseline datasets.\n",
    "        \n",
    "        Args:\n",
    "            dataset_type (str): The dataset type (\"input\" or \"baseline\") of interest to\n",
    "                                obtain list of filesizes.\n",
    "            \n",
    "        Return (list): List of each file's storage size residing in either input or basline datasets. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Set dataset type to obtain dataset type's list of file directories.\n",
    "        filesize_list = []\n",
    "        if dataset_type == 'input':\n",
    "            dir_list = self.input_dirs\n",
    "        if dataset_type == 'baseline':\n",
    "            dir_list = self.baseline_dirs\n",
    "        \n",
    "        # Extract size of each file. \n",
    "        for file_dir in dir_list:\n",
    "            filesize_list.append(Path(self.hpc_dir + file_dir).stat().st_size)\n",
    "\n",
    "        return filesize_list\n",
    "    \n",
    "    def get_resolutions(self, file_df):\n",
    "        \"\"\"\n",
    "        Extract resolutions based on relative directory path.\n",
    "\n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Relative Directory' (mention of resolution values).\n",
    "\n",
    "        Return (pd.DataFrame): A dataframe w/ resolution in C and km format appended. \n",
    "        \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any dataframe\n",
    "        w/ at least the following feature: 'Relative Directory' (mention of resolution\n",
    "        values).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Map C resolution & Km annotation to Km format.\n",
    "        map_res = {48: 200, 96: 100, 192: 50, 384: 25, 768: 13, 1536: 6.5, 3072: 3.25, \n",
    "                   '400': 400, '200': 200, '100': 100, '050': 50, '025': 25, '13km': 13}\n",
    "\n",
    "        # Map Km & C resolution annotation to C resolution format.\n",
    "        map_cres = {'400': 24, '200': 48, '100': 96, '050': 192, '025': 384, '13km': 768,\n",
    "                     48: 48, 96: 96, 192: 192, 384: 384, 768: 768, 1536: 1536, 3072: 3072}\n",
    "        res_list = []\n",
    "        cres_list = []\n",
    "        for idx, row in file_df.iterrows():\n",
    "\n",
    "            if '3072x1536' in row['Relative Directory']:\n",
    "                res_list.append(np.nan)\n",
    "                cres_list.append(np.nan)\n",
    "            elif '768' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res[768])\n",
    "                cres_list.append(map_cres[768])\n",
    "            elif '384' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res[384])\n",
    "                cres_list.append(map_cres[384])\n",
    "            elif '192' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res[192])\n",
    "                cres_list.append(map_cres[192])\n",
    "            elif '96' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res[96])\n",
    "                cres_list.append(map_cres[96])\n",
    "            elif '48' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res[48])\n",
    "                cres_list.append(map_cres[48])\n",
    "            elif '025' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res['025'])\n",
    "                cres_list.append(map_cres['025'])\n",
    "            elif '050' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res['050'])\n",
    "                cres_list.append(map_cres['050'])\n",
    "            elif '400' in row['Relative Directory'] and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res['400'])\n",
    "                cres_list.append(map_cres['400'])\n",
    "            elif '13km' in row['Relative Directory']:\n",
    "                res_list.append(map_res['13km'])\n",
    "                cres_list.append(map_cres['13km'])        \n",
    "            elif '100' in row['Relative Directory']  and 'CPL_FIX/aC' not in row['Relative Directory']:\n",
    "                res_list.append(map_res['100'])\n",
    "                cres_list.append(map_cres['100'])\n",
    "            else:\n",
    "                res_list.append(np.nan)\n",
    "                cres_list.append(np.nan)\n",
    "    \n",
    "        # Append resolution formats to preprocessing dataframe of interest.\n",
    "        file_df['Res (C)'] = cres_list\n",
    "        file_df['Res (km)'] = res_list\n",
    "\n",
    "        return file_df\n",
    "    \n",
    "    def get_data_component(self, file_df):\n",
    "        \"\"\"\n",
    "        Extract data component name based on files' relative directory paths.\n",
    "\n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Relative Directory' (mention of UFS component).\n",
    "\n",
    "        Return (pd.DataFrame): A dataframe w/ UFS component name appended.\n",
    "        \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any \n",
    "        dataframe w/ at least the following feature: 'Relative Directory' (mention \n",
    "        of UFS component).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract data component name.\n",
    "        data_cat = []\n",
    "        for idx, row in file_df.iterrows():\n",
    "            if 'FV3_hafs' in row['Relative Directory']:\n",
    "                data_cat.append(\"FV3_HAFS\")\n",
    "            elif 'FV3_regional' in row['Relative Directory']:\n",
    "                data_cat.append(\"FV3_REGIONAL\")\n",
    "            elif 'fv3_regional' in row['Relative Directory']:\n",
    "                data_cat.append(\"FV3_REGIONAL\")\n",
    "            elif 'FV3_hafs_regional' in row['Relative Directory']:\n",
    "                data_cat.append(\"FV3_HAFS_REGIONAL\")\n",
    "            elif 'FV3' in row['Relative Directory']:\n",
    "                data_cat.append(\"FV3\")\n",
    "            elif 'MOM6' in row['Relative Directory']:\n",
    "                data_cat.append(\"MOM6\")   \n",
    "            elif 'HYCOM_regional' in row['Relative Directory']:\n",
    "                data_cat.append(\"HYCOM_REGIONAL\")\n",
    "            elif 'HYCOM' in row['Relative Directory']:\n",
    "                data_cat.append(\"HYCOM\") \n",
    "            elif 'CICE' in row['Relative Directory']:\n",
    "                data_cat.append(\"CICE\")\n",
    "            elif 'WW3' in row['Relative Directory']:\n",
    "                data_cat.append(\"WW3\")\n",
    "            elif 'CDEPS_ATM' in row['Relative Directory']:\n",
    "                data_cat.append(\"CDEPS_ATM\")\n",
    "            elif 'CDEPS_OCEAN' in row['Relative Directory']:\n",
    "                data_cat.append(\"CDEPS_OCEAN\")\n",
    "            elif 'DATM' in row['Relative Directory']:\n",
    "                data_cat.append(\"DATM\")\n",
    "            elif 'DATM_CDEPS' in row['Relative Directory']:\n",
    "                data_cat.append(\"CDEPS_DATM\")\n",
    "            elif 'CPL' in row['Relative Directory']:\n",
    "                data_cat.append(\"CPL\")\n",
    "            elif 'GOCART' in row['Relative Directory']:\n",
    "                data_cat.append(\"GOCART\")\n",
    "            else:\n",
    "                data_cat.append(np.nan)\n",
    "                \n",
    "        # Append resolution formats to preprocessing dataframe of interest.        \n",
    "        file_df['UFS Component'] = data_cat\n",
    "\n",
    "        return file_df\n",
    "    \n",
    "    def input_or_restart(self, file_df):\n",
    "        \"\"\"\n",
    "        Confirm all \"input\" or \"restart\" data files are in accordance to the UFS guide.\n",
    "\n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Relative Directory.'\n",
    "\n",
    "        Return (pd.DataFrame): A dataframe w/ the classification of a \n",
    "        file being a \"input\" or \"restart\" file appended.\n",
    "        \n",
    "        - Categorization is based on the UFS guide can be found in the following\n",
    "        link: https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html.\n",
    "        To classify a data file as being an \"input\" file, analysis was performed \n",
    "        to determine the UFS Guide I/O section list of input files matched up w/\n",
    "        the input files residing in the RDHPC datasets. During the analysis, the\n",
    "        files listed in the UFS Guide I/O section list of input files per\n",
    "        UFS component were located in \"file_df\" & their directories (shown below)\n",
    "        were captured.\n",
    "        \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any \n",
    "        dataframe w/ at least the following feature: 'Relative Directory.'\n",
    "                \n",
    "        **Follow-up on 020122 w/ NOAA (Denise W. & Jun W.):**\n",
    "        - According to the guide, \"No fix files are required for CICE6,\" \n",
    "        however there is a \"CICE_FIX\" folder. [Follow-up on 020122: Discussion w/ \n",
    "        NOAA, confirmed the files in \"CICE_FIX\" folder does not\n",
    "        follow the UFS Guide. Thus, its foldername could be subject to change -- in\n",
    "        an effort to maintain data structure consistency & comply with the UFS \n",
    "        Guide's definition of static (\"fix\") files.]\n",
    "        \n",
    "        **TODO:** \n",
    "        - Need to acquire the GOCART I/O list of files for confirmation on \n",
    "        its categorization.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Input data folders per UFS component.\n",
    "        fv3_input_folders = ['/FV3_fix', '/FV3_fix_tiled', '/FV3_hafs_input_data', \n",
    "                             'FV3_input_data_C96_with_aerosols', 'FV3_input_data',\n",
    "                             '/FV3_regional_input_data', '/FV3_input_data_regional_esg',\n",
    "                             '/FV3_input_data_gsd', 'FV3_input_frac', 'FV3_input_data_L149',\n",
    "                             '/INPUT', 'ORO_FLAKE', 'FV3_input_data192', 'FV3_input_data384',\n",
    "                             'FV3_input_data48', 'FV3_input_data768', '/FV3_nest_input_data',\n",
    "                             '/FV3_hafs_regional_input_data']\n",
    "        mom6_input_folders = ['/MOM6_FIX', '/MOM6_IC']\n",
    "        hycom_input_folders = ['/HYCOM_regional_input_data']\n",
    "        cice6_input_folders = ['/CICE_FIX', '/CICE_IC']\n",
    "        ww3_input_folders = ['/WW3_input_data'] # ensure this is captured\n",
    "        cdeps_atm_input_folders = ['/DATM_CDEPS/CFSR', '/DATM_CDEPS/GEFS',\n",
    "                                   '/DATM_ERA5_input_data', '/DATM/CFSR/',\n",
    "                                   '/DATM/GEFS/', '/DATM_CDEPS/CFSR3072x1536/201110',\n",
    "                                   '/DATM']\n",
    "        cdeps_ocean_input_folders = ['DOCN_OISST_input_data', 'DOCN_MOM6_input_data']\n",
    "        cpl_input_folders = ['/CPL_FIX']\n",
    "        gocart_input_folders = ['/GOCART']\n",
    "        all_input_folders = fv3_input_folders + mom6_input_folders + hycom_input_folders +\\\n",
    "                            cice6_input_folders + ww3_input_folders +\\\n",
    "                            cdeps_atm_input_folders + cdeps_ocean_input_folders +\\\n",
    "                            cpl_input_folders + gocart_input_folders\n",
    "        \n",
    "        # Extract classification status based on the folders containing \"input\" files\n",
    "        # & files residing w/in a restart (/RESTART) folder.\n",
    "        data_cat = []\n",
    "        for idx, row in file_df.iterrows():\n",
    "            if any(folder in row['Relative Directory'] for folder in all_input_folders):\n",
    "                data_cat.append(\"input\")\n",
    "            elif '/RESTART' in row['Relative Directory']:\n",
    "                data_cat.append(\"restart\")\n",
    "            else:\n",
    "                data_cat.append(np.nan)\n",
    "        \n",
    "        # Append classification status to preprocessing dataframe of interest.     \n",
    "        file_df['input_or_restart'] = data_cat\n",
    "\n",
    "        return file_df\n",
    "    \n",
    "    def get_datatype(self, file_df):\n",
    "        \"\"\"\n",
    "        Extract each data file's format.\n",
    "\n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Filename.'\n",
    "                                    \n",
    "        Return (pd.DataFrame): A dataframe w/ each data file's format appended. \n",
    "                \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any \n",
    "        dataframe w/ at least the following feature: 'Filename.'\n",
    "        \n",
    "        \"\"\"\n",
    "        data_type = []\n",
    "        for idx, row in file_df.iterrows():\n",
    "            if row['Filename'].endswith('.nc1'):\n",
    "                data_type.append(\"nc1\")\n",
    "            elif row['Filename'].endswith('.nc4'):\n",
    "                data_type.append(\"nc4\")\n",
    "            elif row['Filename'].endswith('.nc'):\n",
    "                data_type.append(\"nc\")\n",
    "            elif row['Filename'].endswith('.grb'):\n",
    "                data_type.append(\"grib\")\n",
    "            elif '.Grb' in row['Filename']:\n",
    "                data_type.append(\"grib\")\n",
    "            elif row['Filename'].endswith('.txt'):\n",
    "                data_type.append(\"txt\")\n",
    "            elif row['Filename'].endswith('.f77'):\n",
    "                data_type.append(\"f77\")\n",
    "            elif row['Filename'].endswith('nml'):\n",
    "                data_type.append(\"nml\")\n",
    "            elif row['Filename'].endswith('.dat'):\n",
    "                data_type.append(\"dat\")\n",
    "            elif row['Filename'].endswith('.md5'):\n",
    "                data_type.append(\"md5\")\n",
    "            elif row['Filename'].endswith('.md'):\n",
    "                data_type.append(\"md\")\n",
    "            elif row['Filename'].endswith('.rc'):\n",
    "                data_type.append(\"rc\")\n",
    "            elif row['Filename'].endswith('.res'):\n",
    "                data_type.append(\"res\")\n",
    "            elif row['Filename'].endswith('.a'):\n",
    "                data_type.append(\"32-bit IEEE data\")\n",
    "            elif row['Filename'].endswith('.b'):\n",
    "                data_type.append(\"plain text metadata\")\n",
    "            elif row['Filename'].endswith(tuple(['.SH','.sh'])):\n",
    "                data_type.append(\"shell\")\n",
    "            elif row['Filename'].endswith(tuple(['.TBL','.tbl'])):\n",
    "                data_type.append(\"table\")\n",
    "            elif row['Filename'].endswith(tuple(['.BIN','.bin'])):\n",
    "                data_type.append(\"binary\")\n",
    "            elif row['Filename'].endswith('.ww3'):\n",
    "                data_type.append(\"ww3 binary\")\n",
    "            elif row['Filename'].endswith('.ncl'):\n",
    "                data_type.append(\"ncl\")\n",
    "            else:\n",
    "                data_type.append(np.nan)\n",
    "        \n",
    "        # Append data file formats to preprocessing dataframe of interest. \n",
    "        file_df['DataType'] = data_type\n",
    "        \n",
    "        return file_df\n",
    "    \n",
    "    def get_nodes_feats(self, file_df):\n",
    "        \"\"\"\n",
    "        Extract & partition subfolders into features.\n",
    "\n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Relative Directory.'\n",
    "\n",
    "        Return (pd.DataFrame):  A dataframe w/ each data files' subfolder/node\n",
    "        partitioned into features.\n",
    "        \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any \n",
    "        dataframe w/ at least the following feature: 'Relative Directory.'\n",
    "\n",
    "        \"\"\"\n",
    "        # Extract number of nodes relative to main directory.\n",
    "        file_df['Nodes Relative to Main'] = file_df['Relative Directory'].str.count(\"/\") + 1\n",
    "\n",
    "        # Extract & partition relative directory into sub-directories.\n",
    "        folder_columns_df = file_df['Relative Directory'].str.split('/',expand=True)\n",
    "        lst = range(len(folder_columns_df.columns))\n",
    "        folder_columns_df.columns = [f\"Node{format(x,'d')}\" for x in lst]\n",
    "\n",
    "        # Append subdirectories/node feature columns.\n",
    "        file_df = pd.concat([file_df, folder_columns_df], axis=1)\n",
    "\n",
    "        return file_df\n",
    "    \n",
    "    def get_couple_res(self, dir_df):\n",
    "        \"\"\"\n",
    "        Extract the coupled resolution of coupled data files.\n",
    "        \n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Relative Directory.'\n",
    "                                    \n",
    "        Return (pd.DataFrame): A dataframe w/ coupled resolution of data files appended.\n",
    "        \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any \n",
    "        dataframe w/ at least the following feature: 'Relative Directory.'\n",
    "\n",
    "        \"\"\"\n",
    "        couple_res_list = []\n",
    "        for idx, row in dir_df.iterrows():\n",
    "            if 'CPL_FIX/aC' in row['Relative Directory']:\n",
    "                rel_dir = row['Relative Directory']\n",
    "                atm_res = rel_dir[rel_dir.find('/a') + 1 : rel_dir.find('o')]\n",
    "                ocean_res = rel_dir[rel_dir.find('o') + 1 :]\n",
    "                couple_res_list.append(atm_res + 'o' + ocean_res)\n",
    "            else:\n",
    "                couple_res_list.append(np.nan)\n",
    "        dir_df['Couple Res']= couple_res_list    \n",
    "        \n",
    "        return dir_df\n",
    "    \n",
    "    def cat_restart4baseline(self, file_df):\n",
    "        \"\"\"\n",
    "        Confirm all \"input\" or \"restart\" data files are in accordance to the UFS guide.\n",
    "\n",
    "        Args:\n",
    "            file_df (pd.DataFrame): A dataframe w/ at least the following feature:\n",
    "                                    'Relative Directory.'\n",
    "\n",
    "        Return (pd.DataFrame): A dataframe w/ the classification of a \n",
    "        file being a \"input\" or \"restart\" file appended.\n",
    "        \n",
    "        - Categorization is based on the UFS guide can be found in the following\n",
    "        link: https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html.\n",
    "        To classify a data file as being an \"input\" file, analysis was performed \n",
    "        to determine the UFS Guide I/O section list of input files matched up w/\n",
    "        the input files residing in the RDHPC datasets. During the analysis, the\n",
    "        files listed in the UFS Guide I/O section list of input files per\n",
    "        UFS component were located in \"file_df\" & their directories (shown below)\n",
    "        were captured.\n",
    "        \n",
    "        Note: Method is called by \"preprocess_dirs,\" but can be applied to any \n",
    "        dataframe w/ at least the following feature: 'Relative Directory.'\n",
    "        \n",
    "        **TODO:** \n",
    "        - Need to acquire the Baseline datasets details for confirmation on \n",
    "        its categorization.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract classification status based on files residing w/in a restart (/RESTART) folder.\n",
    "        data_cat = []\n",
    "        for idx, row in file_df.iterrows():\n",
    "            if '/RESTART' in row['Relative Directory']:\n",
    "                data_cat.append(\"restart\")\n",
    "            else:\n",
    "                data_cat.append(np.nan)\n",
    "        \n",
    "        # Append classification status to preprocessing dataframe of interest.     \n",
    "        file_df['input_or_restart'] = data_cat\n",
    "\n",
    "        return file_df\n",
    "\n",
    "    def preprocess_dirs(self, dir_list, dataset_type):\n",
    "        \"\"\"\n",
    "        Apply feature engineering, preprocess & categorize the \"input\" or \"baseline\" dataset files.\n",
    "        \n",
    "        Args:\n",
    "            dir_list (list): List of either input or baseline directories \n",
    "                             (w/ root directory truncated).\n",
    "            dataset_type (str): The dataset type (\"input\" or \"baseline\") to\n",
    "                                extract features & preprocess.\n",
    "            \n",
    "        Return (pd.DataFrame): A dataframe comprised of the filenames & its newly created \n",
    "        features for the given dataset type. Some of the new features will reveal each data\n",
    "        file's root folder, filename, filesize, file format, compiler (if applicable), \n",
    "        CNTL folder (if applicable), \"input\" or \"restart\" file type, resolution (km) category,\n",
    "        resolution (C resolution) category. \n",
    "        \n",
    "        \"\"\"\n",
    "        # Extract subdirectories.\n",
    "        dirs_tokens = [name.split(\"/\") for name in dir_list]\n",
    "        sub_folders = ['/'.join(x[:-1]) for x in dirs_tokens]\n",
    "\n",
    "        # Extract filenames.\n",
    "        filenames = [os.path.basename(x) for x in dir_list]\n",
    "        \n",
    "        # Extract UFS component name, CNTL folder name, filesize, & relevant compiler.\n",
    "        if dataset_type == 'input':\n",
    "            root_folder = [x[1:2][0] for x in dirs_tokens]\n",
    "            filesize = self.input_filesize\n",
    "            fn2dir = {'Root Folder': root_folder, \n",
    "                      'Relative Directory': sub_folders,\n",
    "                      'Filename': filenames,\n",
    "                      'Size': filesize}\n",
    "        if dataset_type == 'baseline':\n",
    "            cntl_folders = [x[2:3][0] for x in dirs_tokens]\n",
    "            filesize = self.baseline_filesize\n",
    "            compiler = [x[1:2][0] for x in dirs_tokens]\n",
    "            fn2dir = {'CNTL Folder': cntl_folders, \n",
    "                      'Compiler': compiler, \n",
    "                      'Relative Directory': sub_folders, \n",
    "                      'Filename': filenames, \n",
    "                      'Size': filesize}\n",
    "       \n",
    "        # Generate & preprocess dataframe containing new features describing the dataset of interest.\n",
    "        dir_df = pd.DataFrame(fn2dir)\n",
    "\n",
    "        # Extract & append each dataset's timestamped date to dataframe.\n",
    "        dir_df['Date'] = dir_df['Relative Directory'].apply(lambda x: re.search(r'\\d{4}\\d{2}\\d{2}', x).group())\n",
    "        dir_df.insert(0, 'Date', dir_df.pop('Date'))\n",
    "        \n",
    "        # Extract & append each file's resolutions (C & km formatted) to dataframe.\n",
    "        dir_df = self.get_resolutions(dir_df)\n",
    "        \n",
    "        # Extract the coupled resolution of coupled data files.\n",
    "        dir_df = self.get_couple_res(dir_df)\n",
    "\n",
    "        # Extract & append each file's format to dataframe.\n",
    "        dir_df = self.get_datatype(dir_df)\n",
    "        \n",
    "        # Extract additional features for the input dataset & append to dataframe.\n",
    "        if dataset_type == 'input':\n",
    "\n",
    "            # Extract & append UFS component name based on rel. directory.\n",
    "            dir_df = self.get_data_component(dir_df)\n",
    "\n",
    "            # Extract & append of \"input\" and \"restart\" status of each file for confirmation.\n",
    "            dir_df = self.input_or_restart(dir_df)   \n",
    "            \n",
    "        # Extract additional features for the baseline dataset & append to dataframe.    \n",
    "        if dataset_type == 'baseline': \n",
    "            # Extract & append of \"input\" and \"restart\" status of each file for confirmation.\n",
    "            dir_df = self.cat_restart4baseline(dir_df)\n",
    "            \n",
    "        # Extract & append subdirectories/node features.\n",
    "        dir_df = self.get_nodes_feats(dir_df)           \n",
    "            \n",
    "        # Save preprocessed dataframe as a pickle file.        \n",
    "        self.save2pickle(dir_df, f'{dataset_type}_df')\n",
    "        \n",
    "        return dir_df\n",
    "    \n",
    "    def get_freq_files(self, input_df, ts_date):\n",
    "        \"\"\"\n",
    "        Extract filenames that occur more than once under a given timestamp & root folder.\n",
    "\n",
    "        Args:\n",
    "            input_df (pd.DataFrame):\n",
    "            ts_date (str): Timestamp to filter input datasets or None if no filtering of input\n",
    "            datasets by timestamp.\n",
    "\n",
    "        Return (pd.DataFrame): Generate dataframe containing only unique files which occur more than once\n",
    "        # under a given timestamp' root folder. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract count of each file per date per root folder.\n",
    "        file_freq_in_root = pd.DataFrame(input_df.groupby(['Date', 'Root Folder', 'Filename'])['Filename'].count())\n",
    "        file_freq_in_root.columns = ['Occurence in Root Folder']\n",
    "\n",
    "        ## Filter to files which occur more than once under a given timestamp' root folder.\n",
    "        # **NOTE: Expect to see tiles given the various levels of subfolders per root folder.\n",
    "        # Focus is on the static files that were repeating\n",
    "        file_freq_in_root = file_freq_in_root[file_freq_in_root['Occurence in Root Folder']>1]\n",
    "\n",
    "        # Extract list of unique files which occur more than once under a given \n",
    "        # timestamp's root folder.\n",
    "        multiple_files_in_root = []\n",
    "        for file in set(file_freq_in_root.index.get_level_values('Filename')):\n",
    "            multiple_files_in_root.append(file)\n",
    "\n",
    "        # Generate dataframe containing only unique files which occur more than once\n",
    "        # under a given timestamp' root folder. \n",
    "        mult_files_df = input_df[input_df['Filename'].isin(multiple_files_in_root)]\n",
    "        if ts_date != None:\n",
    "            mult_files_df = mult_files_df[mult_files_df['Date'] == ts_date] #'20211210', '20211203'\n",
    "        else:\n",
    "            ts_date ='Overall'\n",
    "        mult_files_df = mult_files_df[mult_files_df.duplicated(['Root Folder','Filename','Size'],\\\n",
    "                                                                 keep=False)].sort_values(['Root Folder',\\\n",
    "                                                                                           'Filename',\\\n",
    "                                                                                           'Size'],\\\n",
    "                                                                                           ascending=True)\n",
    "\n",
    "        # Save dataframe of unique files which occur more than once to csv.\n",
    "        mult_files_df.to_csv(f'Multiple_Files_In_Root_{ts_date}.csv', index = False)\n",
    "\n",
    "        return mult_files_df\n",
    "    \n",
    "    def get_freq_files(self, data_df, ts_date, dataset_type):\n",
    "        \"\"\"\n",
    "        Extract filenames occuring more than once under a given dataset type, timestamp, & root/CNTL folder.\n",
    "\n",
    "        Args:\n",
    "            data_df (pd.DataFrame): Dataframe of the input or baseline datasets w/ at least the \n",
    "                                    following features: \"Filename\", \"Date\", \"Size\" & \"Root Folder\" \n",
    "                                    or \"CNTL Folder.\"\n",
    "            ts_date (str): Timestamp to filter input datasets or None if no filtering of input\n",
    "                           datasets by timestamp.\n",
    "            dataset_type (str): \"input\" or \"baseline\" dataset type of \"data_df.\"\n",
    "\n",
    "        Return (pd.DataFrame): Generate dataframe containing only unique files which occur more than once\n",
    "        # under a given timestamp' root folder. \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define dataset's type of main folder (Root vs CNTL).\n",
    "        if dataset_type == 'input':\n",
    "            main_folder4cat = 'Root Folder'\n",
    "        if dataset_type == 'baseline':\n",
    "            main_folder4cat = 'CNTL Folder'\n",
    "\n",
    "        # Extract count of each file per date per root folder.\n",
    "        file_freq_in_root = pd.DataFrame(data_df.groupby(['Date', main_folder4cat, 'Filename'])['Filename'].count())\n",
    "        file_freq_in_root.columns = [f'Occurence in {main_folder4cat}']\n",
    "\n",
    "        ## Filter to files which occur more than once under a given timestamp' root folder.\n",
    "        # **NOTE: Expect to see tiles given the various levels of subfolders per root folder.\n",
    "        # Focus is on the static files that were repeating\n",
    "        file_freq_in_root = file_freq_in_root[file_freq_in_root[f'Occurence in {main_folder4cat}']>1]\n",
    "\n",
    "        # Extract list of unique files which occur more than once under a given \n",
    "        # timestamp's root folder.\n",
    "        multiple_files_in_root = []\n",
    "        for file in set(file_freq_in_root.index.get_level_values('Filename')):\n",
    "            multiple_files_in_root.append(file)\n",
    "\n",
    "        # Generate dataframe containing only unique files which occur more than once\n",
    "        # under a given timestamp' root folder. \n",
    "        mult_files_df = data_df[data_df['Filename'].isin(multiple_files_in_root)]\n",
    "        if ts_date != None:\n",
    "            mult_files_df = mult_files_df[mult_files_df['Date'] == ts_date] #'20211210', '20211203'\n",
    "        else:\n",
    "            ts_date ='Overall'\n",
    "        mult_files_df = mult_files_df[mult_files_df.duplicated([main_folder4cat,'Filename','Size'],\\\n",
    "                                                                 keep=False)].sort_values([main_folder4cat,\\\n",
    "                                                                                           'Filename',\\\n",
    "                                                                                           'Size'],\\\n",
    "                                                                                           ascending=True)\n",
    "\n",
    "        # Save dataframe of unique files which occur more than once to csv.\n",
    "        mult_files_df.to_csv(f'Multiple_filesin{main_folder4cat}_{dataset_type}data{ts_date}.csv', index = False)\n",
    "\n",
    "        return mult_files_df\n",
    "\n",
    "    def get_unique_root_folders(self, df, dataset_type):\n",
    "        \"\"\"\n",
    "        Extracts unique root/CNTL foldernames for each timestamp of a given dataset type.\n",
    "\n",
    "        Args:\n",
    "            df (pd.Dataframe): Dataframe of input or baseline datasets.\n",
    "            dataset_type (str): The dataset type (\"input\" or \"baseline\") to\n",
    "                                extract  root/CNTL foldernames from.\n",
    "\n",
    "        Return (pd.DataFrame): Dataframe of filtered to reveal unique root/CNTL foldernames\n",
    "        for each timestamp. \n",
    "\n",
    "        \"\"\"\n",
    "        ts2root = {}\n",
    "        for ts in set(df['Date']):\n",
    "            if dataset_type == 'input':\n",
    "                ts2root[ts] = list(set(df[df['Date'] == ts]['Root Folder']))\n",
    "            if dataset_type == 'baseline':\n",
    "                ts2root[ts] = list(set(df[df['Date'] == ts]['CNTL Folder']))\n",
    "\n",
    "        return pd.DataFrame.from_dict(ts2root, orient='index').transpose()\n",
    "    \n",
    "    def save2pickle(self, data2save, fn):\n",
    "        \"\"\"\n",
    "        Save data to pickle file.\n",
    "        \n",
    "        Args:\n",
    "            data2save (dict, str, tuple, list, pd.DataFrame): Data to save.\n",
    "            fn (str): Filename for pickle file. \n",
    "        \n",
    "        Return : None\n",
    "        \n",
    "        \"\"\"\n",
    "        with open(fn + '.pkl', 'wb') as file:\n",
    "            pickle.dump(data2save, file)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def read_pickle(self, fn):\n",
    "        \"\"\"\n",
    "        Read data from pickle file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): Filename of pickle file. \n",
    "        \n",
    "        Return (dict, str, tuple, list, pd.DataFrame): Pickle file's data. \n",
    "        \n",
    "        \"\"\"\n",
    "        with open(fn + '.pkl', 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            \n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "## UFS Regression Test Framework Datasets On-Prem Locations\n",
    "\n",
    "| RDHPCS Platform | Dataset Locations |\n",
    "| :- | :- |\n",
    "| __Orion__ | /work/noaa/nems/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __Hera__ | /scratch1/NCEPDEV/nems/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __GAEA__ | /lustre/f2/pdata/ncep_shared/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __Wcoss2__ | /lfs/h1/emc/eib/noscrub/Dusan.Jovic/NEMSfv3gfs |\n",
    "| __Wcoss_dell_p3__ | /gpfs/dell2/emc/modeling/noscrub/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __Wcoss_cray__ | /gpfs/hps3/emc/nems/noscrub/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __Cheyenne__ | /glade/p/ral/jntp/GMTB/ufs-weather-model/RT/NEMSfv3gfs |\n",
    "| __S4__ | /data/users/dhuber/save/nems/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __Jet__ | /lfs4/HFIP/h-nems/emc.nemspara/RT/NEMSfv3gfs |\n",
    "| __Stampede__ | /work2/07736/minsukji/stampede2/ufs-weather-model/RT/NEMSfv3gfs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link Home Directory to Dataset Location on RDHPCS Platform \n",
    "linked_home_dir = \"/home/schin/work\"\n",
    "\n",
    "# UFS RT Dataset Location on RDHPCS Platform \n",
    "orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate wrapper. Filter out dataset folders comprised of a person's name.\n",
    "avoid_fldrs = None\n",
    "data_reader = read_data_dirs(orion_rt_data_dir, avoid_fldrs)\n",
    "\n",
    "# Extract all input file & baseline file directories (root directory truncated).\n",
    "input_dirs, baseline_dirs = data_reader.input_dirs, data_reader.baseline_dirs\n",
    "\n",
    "# Preprocess & extract additonal information on each dataset type.\n",
    "input_df = data_reader.preprocess_dirs(data_reader.input_dirs, dataset_type=\"input\")\n",
    "baseline_df = data_reader.preprocess_dirs(data_reader.baseline_dirs, dataset_type=\"baseline\")\n",
    "\n",
    "# Extracts unique root foldernames for each timestamp in input datasets.\n",
    "root_per_input_ts = data_reader.get_unique_root_folders(input_df, dataset_type=\"input\")\n",
    "\n",
    "# Extracts unique root foldernames for each timestamp in baseline datasets.\n",
    "root_per_bl_ts = data_reader.get_unique_root_folders(baseline_df, dataset_type=\"baseline\")\n",
    "\n",
    "## [Shifting task to next sprint]\n",
    "## TODO: Merging Apps-to-Physics Combinations & their corressponding cntl directory, fv3, and parm file to dataframe.\n",
    "## Read the dataframe comprised of Apps-to-Physics Combinations & their corressponding cntl directory, fv3, and parm file.\n",
    "#appsphys2test_dir = '/home/schin/project_dir/data_scraping/ufs_repo_mapped_data/'\n",
    "#appsphys2test_df = data_reader.read_pickle(appsphys2test_dir + 'rt_appsphys2test_df')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information on UFS Input Datasets \n",
    "Feature were extracted based on:\n",
    "- Location details within each data file's directory path.\n",
    "- UFS Guide I/O section's list of static (\"fixed\") & initial conditions (IC) data files. \n",
    "    - __Latest UFS Guide on I/O:__ https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[input_df['Date']== '20211210']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Directories & Total Directories leading to Data Files for Input Datasets.\n",
    "tot_input_filedirs = len(set(input_df['Relative Directory']))\n",
    "print('\\033[1m'+ f\"\\nTotal Unique Directories leading to Input Data Files:\\033[0m\\n{tot_input_filedirs}\")\n",
    "#print(f\"\\nUnique Directories leading to Input Data File:\\033[0m\\n{set(input_df['Relative Directory'])}\")\n",
    "\n",
    "# Total Timestamped Input Datasets.\n",
    "tot_input_ts = len(set(input_df['Date']))\n",
    "print('\\033[1m'+ f\"\\nTotal Timestamps for Input Datasets:\\033[0m\\n{tot_input_ts}\")\n",
    "\n",
    "# Total Storage Size per Timestamped Input Dataset (GB).\n",
    "size_per_input_gb = input_df.groupby(['Date'])['Size'].sum()/(1024**3)\n",
    "print('\\033[1m'+ f\"\\nStorage Size per Timestamped Input Dataset (GB):\\033[0m\\n{size_per_input_gb}\")\n",
    "\n",
    "# Overall Storage Size of All Timestamped Input Datasets (GB, TB).\n",
    "overall_input_size_gb = round(input_df['Size'].sum()/(1024**3), 1)\n",
    "overall_input_size_tb = round(input_df['Size'].sum()/(1024**4), 2)\n",
    "print('\\033[1m'+ f\"\\nOverall Storage Size of All Timestamped Input Datasets (GB):\\033[0m\\n{overall_input_size_gb}\")\n",
    "print('\\033[1m'+ f\"\\nOverall Storage Size of All Timestamped Input Datasets (TB):\\033[0m\\n{overall_input_size_tb}\")\n",
    "\n",
    "# Unique Input Root Folders.\n",
    "unique_input_root_folders = set(input_df['Root Folder'])\n",
    "print('\\033[1m'+ f\"\\nUnique Root Folders Across All Input Datasets:\\033[0m\\n{unique_input_root_folders}\")\n",
    "\n",
    "# Total Unique Input Root Folders.\n",
    "num_unique_input_root = len(set(input_df['Root Folder']))\n",
    "print('\\033[1m'+ f\"\\nTotal Unique Root Folders Across All Input Datasets:\\033[0m\\n{num_unique_input_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate CSV for Latest Input Data Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Dataset's Timestamp of Interest\n",
    "ts_of_interest = '20211210'\n",
    "dataset_type = 'Input'\n",
    "\n",
    "# Extract Unique Data Files in Input Dataset's Timestamp of Interest.\n",
    "latest_unique_input_files = [input_df[input_df['Date']== ts_of_interest]['Filename'].unique()]\n",
    "latest_unique_input_df = pd.DataFrame(latest_unique_input_files, index = [f'Latest Unique {dataset_type} Data Files (as of {ts_of_interest})']).T \n",
    "\n",
    "# Extract All Data Files in Input Dataset's Timestamp of Interest.\n",
    "latest_input_df = input_df[input_df['Date']== ts_of_interest]\n",
    "\n",
    "# Save Info on Input Dataset's Timestamp of Interest to CSV.\n",
    "with pd.ExcelWriter(f\"Latest_{dataset_type}_Data_Files{ts_of_interest}.xlsx\") as writer:\n",
    "   \n",
    "    # Partition Info on Input Dataset's into Specified CSV sheets.\n",
    "    latest_input_df.to_excel(writer, sheet_name=f\"Latest {dataset_type} Data Files\", index=False)\n",
    "    latest_unique_input_df.to_excel(writer, sheet_name=f\"Latest Unique {dataset_type} Data Files\", index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Folders per Timestamp Input Dataset\n",
    "\n",
    "The number of root folders will provide insight on the input dataset structure's root nodes. Acquiring information on the root nodes per dataset will assist in the reconstruction & mapping of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_per_input_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+ f\"Root Folders per Timestamped Input Dataset:\\033[0m\\n\")\n",
    "root_per_input_ts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine similar root folders between timestamps.\n",
    "input_ts1 = '20211203'\n",
    "input_ts2 = '20211210' \n",
    "list1 = [x for x in root_per_input_ts[input_ts1] if x != None]\n",
    "list2 = [x for x in root_per_input_ts[input_ts2] if x != None]\n",
    "inlist1_notlist2 = sorted(list(set(list1) - set(list2)))\n",
    "inlist2_notlist1 = sorted(list(set(list2) - set(list1)))\n",
    "print('\\033[1m'+ f\"Folder not in {input_ts1}, but in {input_ts2}:\\033[0m\\n{inlist2_notlist1}\")\n",
    "print('\\033[1m'+ f\"\\nFolder not in {input_ts2}, but in {input_ts1}:\\033[0m\\n{inlist1_notlist2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Duplicated Files in Input Datasets\n",
    "\n",
    "A filtering of a list of files occuring more than once of the same file size per root folder and transfering it to a csv file for analysis will provide insights on the what files are being duplicated. An alternative approach to determine the duplicated data files is via hash checksum, however some data files were not readable (user permission denied) to determine duplicates via hash checksum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter to Mutilple Files Dataframe & Save to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_of_interest = None # [Options]: None, set to input dataset timestamp of interest (e.g.'20211210')\n",
    "multiple_files_in_root_df = data_reader.get_freq_files(input_df, ts_of_interest, 'input')\n",
    "multiple_files_in_root_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Current Input Data Structure's Depth Levels\n",
    "\n",
    "The number of nodes (\"folders\") relative to each root folder will provide insights on the current dataset structure's depth. Acquiring information on the bandwidth of the nodes per file relative to their root folder will assist in the reconstruction & mapping of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_freq = pd.DataFrame(input_df['Nodes Relative to Main'].value_counts())\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "node_freq.plot(ax=ax, kind='bar')\n",
    "\n",
    "# Set label style.\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title('Input Datasets:\\nNumber of Files vs Number of Nodes Relative to Root Folder\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_xlabel('\\nNumber of Nodes Relative to Root Folder', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_ylabel('Number of Files \\n', fontsize=16, fontweight='black', color = '#333F4B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[input_df['Nodes Relative to Main']==4]['Root Folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepest_nodes = list(set(input_df[input_df['Nodes Relative to Main']==8]['Root Folder']))\n",
    "print('\\033[1m'+ f\"Root folder(s) with deepest level in folder structure (Total: {len(deepest_nodes)}):\\033[0m\\n\", deepest_nodes)\n",
    "\n",
    "majority_nodes = list(set(input_df[input_df['Nodes Relative to Main']==3]['Root Folder']))\n",
    "print('\\033[1m'+ f\"\\n\\nRoot folders with majority of input files within 3 nodes relative to the main timestamped input folder (Total: {len(majority_nodes)}):\\033[0m\\n\", majority_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Storage Size Distribution (GB) per Timestamped Input Dataset \n",
    "Determining the file storage per timestamped input dataset will provide insights on the current dataset structure's storage size and inform the team on how much storage will be needed for the UFS input datasets for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizeperdate = pd.DataFrame(input_df.groupby(['Date'])['Size'].sum()/(1024**3))\n",
    "\n",
    "# Variation in storage size per input dataset.\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "input_sizeperdate.plot(kind='barh', legend = False, color= (0, 0.4, 0.9, 0.5), ax=ax, alpha=0.4, linewidth=2)\n",
    "\n",
    "# Set label style.\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title('Timestamped Input Dataset vs Size (GB)\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_xlabel('\\nSize (GB)', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_ylabel('Timestamped Input Dataset\\n', fontsize=16, fontweight='black', color = '#333F4B')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Storage Size Distribution (GB) per UFS Component per Timestamped Input Dataset \n",
    "Determining the file storage per UFS component per timestamped input dataset will provide insights on the current dataset structure's storage size by UFS component and can assist in detecting any potential outliers/anomalies within a given UFS component input data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage size conversion to GB per UFS component.\n",
    "input_sizepercomp = pd.DataFrame(input_df.groupby(['Date', 'UFS Component'])['Size'].sum()/(1024**3))\n",
    "\n",
    "# Distribution in storage size per input dataset per component.\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "input_sizepercomp.plot(kind='barh', legend = False, color= (0, 0.6, 0, 0), ax=ax, alpha=0.9, linewidth=6)\n",
    "\n",
    "# Set label style.\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.set_title('(Timestamped Input Dataset, UFS Component) vs Size (GB)\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_xlabel('\\nSize (GB)', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_ylabel('(Timestamped Input Dataset, UFS Component)\\n', fontsize=16, fontweight='black', color = '#333F4B')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One root folder not UFS component labelled, but are input data (OISST) from the satellite office of NOAA (NESDIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Folders not UFS component labelled, but are input data (OISST) from the satellite office of NOAA (NESDIS)\n",
    "print('\\033[1m'+ f\"Folders labelled as UFS components presented in input datasets:\\033[0m\\n{set(input_df['UFS Component'])}\")\n",
    "input_df[input_df['UFS Component'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (as of 02/03/22): UFS Input Datasets\n",
    "\n",
    "- After cleaning of the input datasets, the new input dataset storage size reserved on Orion has decreased from 500GB to 350GB and the file size distribution between the timestamped datasets are similar to each other with a negligible difference in storage size overall since, last BDP meeting (on 01/31/22).\n",
    "\n",
    "- After cleaning of the input datasets, there are duplicated data files within the same directory path of a given timestamped input dataset. If files of similar name and size occur more than once within a root folder, then the files are seperated by subfolders at deeper node levels as they are dependent on resolutions.\n",
    "\n",
    "- There are 2 timestamped input datasets and 208 unique directory paths leading to baseline data files.\n",
    "- There are 33 unique root folders across all input datasets. Majority of these root folders are UFS component labelled with one root folder labelled as __DOCN_OISST_input_data__ (\"input data (OISST) from the satellite office of NOAA (NESDIS)\").\n",
    "\n",
    "- Root folder with the largest number of nodes (\"deepest level\") with 8 nodes relative to main timestamped input folder in depth:\n",
    "    - GOCART\n",
    "    \n",
    "- Root folders featuring the next largest number of nodes with 4 nodes relative to main timestamped input folder in depth:\n",
    "    - DATM\n",
    "    - DATM_CDEPS\n",
    "    - FV3_input_data_INCCN_aeroclim\n",
    "    - MOM6_IC\n",
    "    \n",
    "- Majority of the files are located within 3 nodes relative to the main timestamped input folder. There are 25 root folders with at least 3 nodes relative to the main timestamped input folder in depth.\n",
    "    - CICE_IC\n",
    "    - WW3_input_data_20211113\n",
    "    - fv3_regional_c768\n",
    "    - FV3_input_data384\n",
    "    - FV3_input_data_regional_esg\n",
    "    - FV3_fix_tiled\n",
    "    - FV3_input_data192\n",
    "    - FV3_input_data_sar\n",
    "    - MOM6_FIX_DATM\n",
    "    - FV3_hafs_input_data\n",
    "    - FV3_input_data_conus13km\n",
    "    - FV3_input_data768\n",
    "    - CICE_FIX\n",
    "    - CPL_FIX\n",
    "    - FV3_input_frac\n",
    "    - fv3_regional_control\n",
    "    - FV3_fix\n",
    "    - FV3_input_data_L149\n",
    "    - MOM6_FIX\n",
    "    - FV3_input_data_gsd\n",
    "    - FV3_input_data48\n",
    "\n",
    "- All root folders are UFS component labelled. There is only one root folder, __DOCN_OISST_input_data__, which is not a UFS component and contains  input data (OISST) gathered from the satellite office of NOAA (NESDIS).\n",
    "\n",
    "### Findings (as of 02/03/22):\n",
    "\n",
    "- __$\\color{blue}{\\text{[Sent Findings to Jun on 02/03/22 via Email]}}$__ Although its file is relatively small, would it be reasonable to not have them in different subfolders, but rather one subfolder within their respective root directory -- since __grid_spec.nc__ is not specific to the number of vertical levels? For example, placing __grid_spec.nc__ into a single location (in this case, for the input-data-20211210's FV3_input_data384 resolution, the input-data-20211210/FV3_input_data384/INPUT folder) -- rather than in two or three different subfolders per unique root path as shown below:\n",
    "\n",
    "    - input-data-20211210/FV3_input_data384/INPUT/__grid_spec.nc__ (~22k)\n",
    "    - input-data-20211210/FV3_input_data384/INPUT_L127/__grid_spec.nc__ (~22k)\n",
    "    - input-data-20211210/FV3_input_data384/INPUT_L127_GDAS/__grid_spec.nc__ (~22k)\n",
    "\n",
    "    - input-data-20211210/FV3_input_data/INPUT_L127/__grid_spec.nc__ (~22k)\n",
    "    - input-data-20211210/FV3_input_data/INPUT/__grid_spec.nc__ (~22k)\n",
    "\n",
    "    - input-data-20211210/FV3_input_data192/INPUT_L127/__grid_spec.nc__ (~22k)\n",
    "    - input-data-20211210/FV3_input_data192/INPUT/__grid_spec.nc__ (~22k)\n",
    "    \n",
    "> __Answer__\n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/03/22 via Email]}}$__\n",
    "\"yes, those  __grid_spec.nc__ are the same for the same resolution.  Currently we put all the INPUT data  in one of these INPUT directories. We can move them one level up, but that requires script change as you pointed out, also developers may be confused that the INPUT data are not stay in the same place. So we can change it, but need some time to convey the information to the developers. \n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/07/22 via Email]}}$__\n",
    "I discussed the [Finding 1 on input data] with my team who developed the RT. Several comments are: these __grid_spec.nc__ files are small, moving them outside the INPUT directory where they belong will make the RT script complicated. We need to let all developers be aware of this change before making the change.\" \n",
    "\n",
    "> __$\\color{magenta}{\\text{[Conclude]}}$__\n",
    "If deemed necessary when restructuring UFS datasets, we can allocate these __grid_spec.nc__ file as one __grid_spec.nc__ file to one directory since, there is no reason for them to be categorized into resolutions as they are not dependent on resolution. The tradeoff is between a small amount of duplicate files vs the time developers have to perform to make the changes within the the RT script which may not be worthwhile considering the tradeoff for this change.\" \n",
    "\n",
    "\n",
    "- __$\\color{blue}{\\text{[Sent Findings to Jun on 02/03/22 via Email]}}$__ Timestamped __20211203__ & __20211210__ __input datasets__ differ such that:\n",
    "\n",
    "    - __20211210 input dataset__ contains folder, __/FV3_input_data_conus13km__, whereas __20211203 input dataset__ does not contain folder, __/FV3_input_data_conus13km__.\n",
    "    - __20211203 input dataset__ contains folder, __/DATM__, whereas __20211210 input dataset__ does not contain folder, __/DATM__.\n",
    "\n",
    "> __Answer__\n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/03/22 via Email]}}$__\n",
    "\"the changes are from recent PRs. A new test was added, the input data is FV3_input_data_conus13km. Also some old DATM tests were removed.\" \n",
    "\n",
    "- __$\\color{blue}{\\text{[Sent Findings to Jun on 02/03/22 via Email]}}$__ The deepest data folder structure is the GOCART with a maximum level of 8 nodes (subfolders) relative to its main directory in depth. The majority of the data files within the input dataset structure hovers around 3 nodes relative to its main directory in depth.\n",
    "\n",
    ">__Answer__\n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/03/22 via Email]}}$__\n",
    "\"this may need some work to reduce the depth, I need to talk to the developers.\"\n",
    "\n",
    "> __Follow-up (02/14/22)__\n",
    "\n",
    "> __$\\color{blue}{\\text{[Response to Jun on 02/14/22 via Email]}}$__ Noted. Also, the data analytics pythonic tool that I have created to observe the current structure & details of the UFS datasets is capable of providing your team an overview of what is currently residing within GOCART's dataset.\n",
    "\n",
    "> __$\\color{magenta}{\\text{[Conclude]}}$__ If needed, the tool could transport these GOCART dataset details in a categorical fashion for your team to a .csv spreadsheet or formatted table if desired (as oppose to your team logging in & recording what data files are currently residing for GOCART as well as any other UFS component data files of interest). __**For a reference on the findings for GOCART as of last sprint, documentation is within the following link:__ https://drive.google.com/drive/folders/12egYWleyqaWu9BDIDsPSG9j_fIQ8Ob1b\n",
    "\n",
    "\n",
    "### Findings (as of 02/01/22):\n",
    "- __$\\color{green}{\\text{[RESOLVED]}}$__ There is only one folder ( __/FV3_hafs_input_data__) in Timestamped __20210930 input dataset__. \n",
    "    - __[Follow-up w/ NOAA on 02/03/22]__ Removal of Timestamped __20210930 input dataset__ on Orion by NOAA resolved issue. \n",
    " \n",
    "\n",
    "### Findings (as of 01/31/22):\n",
    "\n",
    "\n",
    "- __$\\color{green}{\\text{[RESOLVED]}}$__ Inconsistency in nomenclature of the input files containing grid information and the initial conditions listed and described in Table 4.2 -- they are described as having the following format: __sfc_data.tile[1-6,7].nc__, __gfs_data.tile[1-6,7].nc__, __gfs_bndy.tile[1-6,7].nc__, __oro_data.tile[1-6,7].nc__, \n",
    "__gfs_ctrl.nc__, __sfc_ctrl.nc__, __C[RES]_grid.tile[1-6,7].nc__.\n",
    "    - __Example:__ __oro_data_ss.tile1.nc__\t\n",
    "    - __[Follow-up w/ NOAA (Jun W.) on 02/01/22]__ Agreed that the filenames will not change, but the folder names can be subject to change if needed during data restructuring.\n",
    "\n",
    "\n",
    "- __$\\color{green}{\\text{[RESOLVED]}}$__ According to the UFS guide (https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html), \"No fix files are required for CICE6,\" however there is a __/CICE_FIX__ folder. Why is there a CICE_FIX folder ? Is this a folder containing grid information data files which NOAA considers as literally a fix file? \n",
    "    - __[Follow-up w/ NOAA (Jun W. & Denise W.) on 02/01/22]:__ Discussion w/ NOAA, confirmed the files in \"CICE_FIX\" folder does not follow the UFS Guide's definition of static \"fix\" data file. Thus, its foldername could be subject to change -- in an effort to maintain data structure consistency & comply with the UFS Guide's definition of static (\"fix\") files. Agreed that when data is being restructured that it complies with the UFS guide: https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html]__\n",
    "\n",
    "\n",
    "- __$\\color{green}{\\text{[RESOLVED]}}$__ Some of static input files will either reside outside or within the '/INPUT' folder. Sol: Will need to re-structure this for consistency. \n",
    "    - __Example:__ __co2historicaldata_YYYY.txt__ & __aerosol.dat__ will either be outside or within the '/INPUT' folder.\n",
    "    - __[Follow-up w/ NOAA on 02/01/22]__ Cleaning & removal of timestamped input dataset files on Orion by NOAA resolved issue. \n",
    "    \n",
    "\n",
    "- __$\\color{darkorange}{\\text{[DETAILS PENDING]}}$__ Per https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html, \"GOCART information in progress\". Need to determine which GOCART related data files are inputs. Need to acquire the GOCART I/O list of files for confirmation on \n",
    "for its categorization.    \n",
    "    - __[Follow-up w/ NOAA (Jun W.) on 02/01/22]:__ Agreed that GOCART data will not be analyze or restructured until information regarding its data is provided by NOAA and/or UFS guide.\n",
    "        \n",
    "    \n",
    "### Notes\n",
    "- The following files are specific to the number of either the vertical and/or horizontal resolution levels \n",
    "    - __gfs_ctrl.nc__  Defines vertical coordinate so, dependent on the vertical resolution, but not horizontal resolution. \n",
    "    - __gfs_data.nc__ (IC) dependent on the vertical resolution. 3-dimensional ICs (lat,lon,vertical height).\n",
    "    - __sfc_data.nc__ (IC) dependent on the vertical resolution.\n",
    "    - __oro_data.nc__ dependent on the vertical resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information on Baseline Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Directories & Total Directories leading to Data Files for Input Datasets.\n",
    "tot_baseline_filedirs = len(set(baseline_df['Relative Directory']))\n",
    "print('\\033[1m'+ f\"\\nTotal Unique Directories leading to Baseline Data Files:\\033[0m\\n{tot_baseline_filedirs}\")\n",
    "#print(f\"\\nUnique Directories leading to Baseline Data File:\\033[0m\\n{set(baseline_df['Relative Directory'])}\")\n",
    "\n",
    "# Total Timestamped Baseline Datasets.\n",
    "tot_baseline_ts = len(set(baseline_df['Date']))\n",
    "print('\\033[1m'+ f\"\\nTotal Timestamps for Baseline Datasets:\\033[0m\\n{tot_baseline_ts}\")\n",
    "\n",
    "# Total Storage Size per Timestamped Baseline Dataset (GB).\n",
    "size_per_baseline_gb = baseline_df.groupby(['Date'])['Size'].sum()/(1024**3)\n",
    "print('\\033[1m'+ f\"\\nStorage Size per Timestamped Baseline Dataset (GB):\\033[0m\\n{size_per_baseline_gb}\")\n",
    "\n",
    "# Overall Storage Size of All Timestamped Baseline Datasets (GB, TB).\n",
    "overall_baseline_size_gb = round(baseline_df['Size'].sum()/(1024**3), 1)\n",
    "overall_baseline_size_tb = round(baseline_df['Size'].sum()/(1024**4), 2)\n",
    "print('\\033[1m'+ f\"\\nOverall Storage Size of All Timestamped Baseline Datasets (GB):\\033[0m\\n{overall_baseline_size_gb}\")\n",
    "print('\\033[1m'+ f\"\\nOverall Storage Size of All Timestamped Baseline Datasets (TB):\\033[0m\\n{overall_baseline_size_tb}\")\n",
    "\n",
    "# Unique Baseline CNTL Folders.\n",
    "unique_baseline_root_folders = set(baseline_df['CNTL Folder'])\n",
    "print('\\033[1m'+ f\"\\nUnique CNTL Folders Across All Baseline Datasets:\\033[0m\\n{unique_baseline_root_folders}\")\n",
    "\n",
    "# Total Unique Baseline CNTL Folders.\n",
    "num_unique_baseline_root = len(set(baseline_df['CNTL Folder']))\n",
    "print('\\033[1m'+ f\"\\nTotal Unique CNTL Folders Across All Baseline Datasets:\\033[0m\\n{num_unique_baseline_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate CSV for Latest Baseline Data Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Dataset's Timestamp of Interest\n",
    "ts_of_interest = '20220224'\n",
    "dataset_type = 'Baseline'\n",
    "\n",
    "# Extract Unique Data Files in Input Dataset's Timestamp of Interest.\n",
    "latest_unique_bl_files = [baseline_df[baseline_df['Date']== ts_of_interest]['Filename'].unique()]\n",
    "latest_unique_bl_df = pd.DataFrame(latest_unique_bl_files, index = [f'Latest Unique {dataset_type} Data Files (as of {ts_of_interest})']).T \n",
    "\n",
    "# Extract All Data Files in Input Dataset's Timestamp of Interest.\n",
    "latest_bl_df = baseline_df[baseline_df['Date']== ts_of_interest]\n",
    "\n",
    "# Save Info on Input Dataset's Timestamp of Interest to CSV.\n",
    "with pd.ExcelWriter(f\"Latest_{dataset_type}_Data_Files{ts_of_interest}.xlsx\") as writer:\n",
    "   \n",
    "    # Partition Info on Input Dataset's into Specified CSV sheets.\n",
    "    latest_bl_df.to_excel(writer, sheet_name=f\"Latest {dataset_type} Data\", index=False)\n",
    "    latest_unique_bl_df.to_excel(writer, sheet_name=f\"Latest Unique {dataset_type} Data\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latest Baseline Files which resides outside of /RESTART\n",
    "\n",
    "Baseline /RESTART files not in current S3:\n",
    "\n",
    "- MOM.res_4.nc\n",
    "- DATM_CFSR3072x1536.cpl.r.2011-10-02-00000.nc\n",
    "- ufs.cpld.cpl.r.2021-03-23-21600.nc\n",
    "- iced.2021-03-23-21600.nc\n",
    "- DATM_CFSR.cpl.r.2011-10-01-43200.nc\n",
    "- iced.2011-10-01-43200.nc\n",
    "- ufs.cpld.cpl.r.2013-04-01-21600.nc\n",
    "- iced.2013-04-01-21600.nc\n",
    "- DATM_GEFS.cpl.r.2011-10-02-00000.nc\n",
    "- fv_tracer.res.tile1_new.nc\n",
    "- fv_core.res.tile1_new.nc\n",
    "- DATM_GEFS.cpl.r.2011-10-01-43200.nc\n",
    "- iced.2011-10-01-21600.nc\n",
    "- DATM_CFSR.cpl.r.2011-10-01-21600.nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest Baseline Files which resides outside of /RESTART\n",
    "baseline_df[(baseline_df['Date'] == '20220224') & (baseline_df['input_or_restart'] != 'restart')]['Filename'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNTL Folders per Timestamp Baseline Dataset\n",
    "\n",
    "The number of CNTL folders will provide insight on the baseline dataset structure's root nodes. Acquiring information on the root nodes per dataset will assist in the reconstruction & mapping of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_per_bl_ts.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+ f\"Root Folders per Timestamped Baseline Dataset:\\033[0m\\n\")\n",
    "root_per_bl_ts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine similar CNTL folders between timestamps.\n",
    "input_ts1 = '20220112'\n",
    "input_ts2 = '20220107' \n",
    "list1 = [x for x in root_per_bl_ts[input_ts1] if x != None]\n",
    "list2 = [x for x in root_per_bl_ts[input_ts2] if x != None]\n",
    "inlist1_notlist2 = sorted(list(set(list1) - set(list2)))\n",
    "inlist2_notlist1 = sorted(list(set(list2) - set(list1)))\n",
    "print('\\033[1m'+ f\"Folder not in {input_ts1}, but in {input_ts2}:\\033[0m\\n{inlist2_notlist1}\")\n",
    "print('\\033[1m'+ f\"\\nFolder not in {input_ts2}, but in {input_ts1}:\\033[0m\\n{inlist1_notlist2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Duplicated Files in Baseline Datasets\n",
    "\n",
    "A filtering of a list of files occuring more than once of the same file size per cntl folder and transfering it to a csv file for analysis will provide insights on the what baseline files are being duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter to Mutilple Files Dataframe & Save to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_of_interest_bl = None # [Options]: None, set to baseline dataset timestamp of interest (e.g.'20210208', '20210212')\n",
    "multiple_files_in_cntl_df = data_reader.get_freq_files(baseline_df, ts_of_interest_bl, 'baseline')\n",
    "multiple_files_in_cntl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Current Baseline Data Structure's Depth Levels\n",
    "\n",
    "The number of nodes (\"folders\") relative to each root folder will provide insights on the current dataset structure's depth. Acquiring information on the bandwidth of the nodes per file relative to their root folder will assist in the reconstruction & mapping of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_freq_bl = pd.DataFrame(baseline_df['Nodes Relative to Main'].value_counts())\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "node_freq_bl.plot(ax=ax, kind='bar')\n",
    "\n",
    "# Set label style.\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title('Baseline Datasets:\\nNumber of Files vs Number of Nodes Relative to CNTL Folder\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_xlabel('\\nNumber of Nodes Relative to CNTL Folder', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_ylabel('Number of Files \\n', fontsize=16, fontweight='black', color = '#333F4B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepest_nodes_bl = list(set(baseline_df[baseline_df['Nodes Relative to Main']==4]['CNTL Folder']))\n",
    "print('\\033[1m'+ f\"CNTL folder(s) with deepest level in folder structure (Total: {len(deepest_nodes_bl)}):\\033[0m\\n\", deepest_nodes_bl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Storage Size Distribution (GB) per Timestamped Baseline Dataset \n",
    "Determining the file storage per timestamped baseline dataset will provide insights on the current dataset structure's storage size and inform the team on how much storage will be needed for the UFS baseline datasets for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sizeperdate = pd.DataFrame(baseline_df.groupby(['Date'])['Size'].sum()/(1024**3))\n",
    "\n",
    "# Variation in storage size per baseline dataset.\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "baseline_sizeperdate.plot(kind='barh', legend = False, color= (0, 0.4, 0.9, 0.5), ax=ax, alpha=0.4, linewidth=10)\n",
    "\n",
    "# Set label style.\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title('Timestamped Baseline Dataset vs Size (GB)\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_xlabel('\\nSize (GB)', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_ylabel('Timestamped Baseline Dataset\\n', fontsize=16, fontweight='black', color = '#333F4B')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Storage Size Distribution (GB) per CNTL Folder Called by Regression \"/tests\" Files.\n",
    "Determining the file storage per CNTL foldername called by Regression Test Framework's \"/tests\" Files will provide insights on a given baseline dataset's storage size required for a given regression test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Size Distribution (GB) per CNTL Folder Called by Regression \"/tests\" Files of Latest Baseline Dataset ('20220113'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage size conversion to GB per CNTL folder.\n",
    "baseline_sizepercomp = pd.DataFrame(baseline_df.groupby(['Date', 'CNTL Folder'])['Size'].sum()/(1024**3))\n",
    "\n",
    "# Latest baseline size per CNTL folder.\n",
    "repo_date = '20220113'\n",
    "latest_baseline_sizepercntl = baseline_sizepercomp.xs(repo_date, drop_level=True)\n",
    "\n",
    "# Distribution in storage size per CNTL folder.\n",
    "fig, ax = plt.subplots(figsize=(12,20))\n",
    "latest_baseline_sizepercntl.plot(kind='barh', legend = False, color= (0, 0, 0.5, 0), ax=ax, alpha=0.7, linewidth=6)\n",
    "\n",
    "# Set label style.\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title(f'Latest Timestamped Baseline Dataset {repo_date}:\\n CNTL Called by Regression \"/tests\" Files vs Size (GB)\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_xlabel('\\nSize (GB)', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "ax.set_ylabel(f'Latest Timestamped Baseline Dataset CNTL Called by Regression \"/tests\" Files.\\n', fontsize=16, fontweight='black', color = '#333F4B')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (as of 02/03/22): UFS Baseline Datasets\n",
    "\n",
    "- Currently, the dataset storage size reserved on Orion is approximately 6 TB. the file size distribution between the timestamped datasets deviate from each other, in which most of the baseline datasets are approx. 350 GB, but 20210208 and 20210212 basleine datasets are only approx. 100 GB.\n",
    "\n",
    "- There are 19 timestamped baseline datasets and 1891 unique directory paths leading to baseline data files.\n",
    "\n",
    "- There are 126 unique CNTL folders across all baseline datasets. These CNTL folders are called by the UFS weather model repository's Regression Test framework in files reserved in \"tests/tests\" folder (https://github.com/ufs-community/ufs-weather-model/tree/develop/tests/tests).\n",
    "\n",
    "- A given baseline datasets' largest number of nodes (\"deepest level\") is 4 nodes.\n",
    "    \n",
    "- The majority of the baseline data files are located within 4 nodes relative to the main timestamped baseline folder. \n",
    "\n",
    "- There are 59 CNTL folders with at least 4 nodes relative to the main timestamped baseline folder in depth.\n",
    "\n",
    "\n",
    "### Findings (as of 02/03/22): \n",
    "\n",
    "- __$\\color{blue}{\\text{[Sent Findings to Jun via Email on 02/03/22]}}$__ As of 02/03/22, there are no duplicated baseline data files within the same directory path of a given timestamped baseline dataset. However, there are a few data files that have the same name and size which are occurring more than once within a baseline’s CNTL folder, and they are those files that are separated between the __/INPUT__ and __/RESTART__ subfolders for only two timestamped baseline datasets (20210208 and 20210212). Some of these files are:\n",
    "\n",
    "    - aerosol.dat\n",
    "    - co2historicaldata_YYYY\n",
    "    - co2monthlycyc.txt \n",
    "    - solarconstant_noaa_an.txt\n",
    "    - sfc_emissivity_idx.txt\n",
    "    - C96_grid.tile#.nc\n",
    "    - oro_data.tile#.nc\n",
    "    - global_o3prdlos.f77\n",
    "    - grid_spec.nc\n",
    "\n",
    "Although the overall storage sizes of these files is minimal, that space that could be saved would be 24MB per the aforementioned timestamps if their duplicate file was removed and place in a single subfolder rather than two subfolders of a given CNTL folder.\n",
    "\n",
    "> __Answer__\n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/03/22 via Email]}}$__\n",
    "\"baseline, the __ufs-public-release-v2-20210208__ and __ufs-public-release-v2-20210212__ were generated before we reorganized the baseline data and the input data. As you see, the input data and the regression test baselines stayed in the same directory, and the input data were not cleaned up. And the corresponding model code version is almost one year old, I won't suggest looking at them.\" \n",
    "\n",
    "\n",
    "> __Follow-ups (02/14/22)__\n",
    "\n",
    "> __$\\color{blue}{\\text{[Sent to Jun on 02/23/22 via Email]}}$__ •\tFrom your previous email, you mentioned: 'baseline, the ufs-public-release-v2-20210208 and ufs-public-release-v2-20210212 were generated before we reorganized the baseline data and the input data. As you see, the input data and the regression test baselines stayed in the same directory, and the input data were not cleaned up. And the corresponding model code version is almost one year old, I won't suggest looking at them.' Based on your previous email and my findings: If ufs-public-release-v2-20210212 is to be ignored, then should only the following baseline datasets be ignored as well ?\"\n",
    "- ufs-v2.0.0: ufs-public-release-v2-20210212\n",
    "- ufs-v1.1.0: ufs-public-release-v2-20200728\n",
    "- ufs-v1.0.0: ufs-public-release-v2-20200224\n",
    "\n",
    "> __Answer__\n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/23/22 via Email]}}$__\n",
    "\"1) Yes. I'd suggest focusing on the new release\"\n",
    "\n",
    "> __$\\color{magenta}{\\text{[Conclude]}}$__\n",
    "Consider focusing on all UFS repository code release versions ( \"new release\" versions) on GitHub, which does not include: \n",
    "- ufs-v2.0.0: ufs-public-release-v2-20210212\n",
    "- ufs-v1.1.0: ufs-public-release-v2-20200728\n",
    "- ufs-v1.0.0: ufs-public-release-v2-20200224\n",
    "\n",
    "> __$\\color{blue}{\\text{[Sent to Jun on 02/23/22 via Email]}}$__ •\tAs I create a tool for which scrapes the current Github release version names to their respective baseline and input dataset timestamps, I noticed that __ufs-public-release-v2-20210212__ is being utilized for __'ufs-v2.0.0'__ release version of the UFS weather model repository. Does that mean that we should not take into account the dataset needed for  __'ufs-v2.0.0'__ release version of the UFS weather model repository and even perhaps those release versions older than  __'ufs-v2.0.0'__ ?\n",
    "    > - In other words, what release versions of the UFS weather model repository should our team only be focused on for this year?\n",
    " \n",
    "![input2codeversion](img/ufs_input2release_version.png)\n",
    "![baseline2codeversion](img/ufs_baseline2release_version.png)   \n",
    "\n",
    "\n",
    "> __Answer__\n",
    "\n",
    "> __$\\color{darkgreen}{\\text{[Response from Jun on 02/23/22 via Email]}}$__\n",
    "\"2) I am not sure how this will be done this year. But for the previous releases, there was a release team formed, they decided the scope of the release, when all the required changes were available in the develop branch, we created a release branch out of the develop branch. So far I am not aware of the scope of the release yet.\"\n",
    "\n",
    "- __$\\color{blue}{\\text{[Sent Findings to Jun via Email on 02/03/22]}}$__ Some of the timestamped baseline datasets differ in the number of CNTL folders that they posses. Should this be expected?\n",
    "\n",
    "    - Folder not in 20220112, but in 20220107: 'control_debug_p7', 'control_p7', 'cpld_control_c192_p7', 'cpld_control_c384_p7', 'cpld_control_c96_p7', 'cpld_control_p7', 'cpld_debug_p7'\n",
    "\n",
    "    - Folder not in 20220107, but in 20220112: 'control_debug_p8', 'control_p8', 'cpld_bmark_p8', 'cpld_control_c192_p8', 'cpld_control_c384_p8', 'cpld_control_c96_p8', 'cpld_control_p8', 'cpld_debug_p8'    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Data Storage Size of UFS Data Residing on Orion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+ f\"\\nOverall Data Storage Size of UFS Data Residing on Orion (TB):\\033[0m\\n{round((input_df['Size'].sum()+baseline_df['Size'].sum())/(1024**4), 1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Overview\n",
    "\n",
    "Since our last BDP meeting (01/31/22), the overall dataset storage size (including all input & baseline datasets) has decreased from ~ 6.5TB to 6.3TB. The input dataset storage size reserved on Orion has decreased from ~500GB to 350GB, but the baseline dataset storage size remains the same at ~6TB. Data analysis was performed to ...\n",
    "\n",
    "1) Detect any additional duplicated files in the input and baseline datasets. As of today (02/02/22), there are no longer duplicate files residing within the input datasets -- however there are duplicate files within the baseline datasets which only occurs within 2 (out of the 19) timestamp baseline datasets, but they are separated by /INPUT and /RESTART subfolders. Some of these files are:\n",
    "\n",
    "- aerosol.dat\n",
    "- co2historicaldata_YYYY\n",
    "- co2monthlycyc.txt \n",
    "- solarconstant_noaa_an.txt\n",
    "- sfc_emissivity_idx.txt\n",
    "- C96_grid.tile#.nc\n",
    "- oro_data.tile#.nc\n",
    "- global_o3prdlos.f77\n",
    "- grid_spec.nc\n",
    "\n",
    "2) Determine the file storage per UFS component per timestamped input dataset in an effort to acquire insight on the current input dataset structure's storage size for any given UFS component. Purpose: Can assist the team in detecting any potential outliers/anomalies within a given UFS component input's data structure. \n",
    "\n",
    "3) Determine the file storage per timestamped input dataset in an effort to acquire insight on the current dataset structure's storage size. Purpose: Can inform team of how much storage will be needed for the UFS input datasets for deployment.\n",
    "\n",
    "4) Determine the file storage per timestamped baseline dataset in an effort to acquire insight on the current dataset structure's storage size. Purpose: Can inform team of how much storage will be needed for the UFS baseline datasets for deployment.\n",
    "\n",
    "5) Determine the number of nodes (\"folders\") relative to each root folder will provide insights on the current dataset structure's depth for both input and baseline datasets. Acquiring information on the bandwidth of the nodes per file relative to their root/CNTL folder will assist in the reconstruction & mapping of the data files.\n",
    "\n",
    "   All in all, data analysis of the current data struture needed to be performed to assist in the cleaning of the current data struture in preparation for mapping the UFS datasets to the RT and UFS application frameworks. In addition, the creation of this pythonic tool will be able to assist the team in the data analytics & mapping of the UFS datasets. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_scraper",
   "language": "python",
   "name": "data_scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
